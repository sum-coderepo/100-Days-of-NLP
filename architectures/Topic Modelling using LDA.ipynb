{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modelling using LDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQBRHTGVyi+WoJy72y1p3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/architectures/architectures/Topic%20Modelling%20using%20LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxxJbporJcjX",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9_CXVBp2A1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "352d8fdc-ce27-4ab2-84a8-05dbb5164b9b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMAraFGB0X8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xmE2NKk1Mp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9TLsH6JJfSZ",
        "colab_type": "text"
      },
      "source": [
        "### Newsgroup dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H17rPEu1lwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "0f860f01-0911-4cc4-87b6-8944de324086"
      },
      "source": [
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ...           target_names\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...              rec.autos\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...  comp.sys.mac.hardware\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...  comp.sys.mac.hardware\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...          comp.graphics\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...              sci.space\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvEzhRI52_NI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "02f8e32c-e564-494b-e9b1-2b246a6e6d40"
      },
      "source": [
        "df.target_names.unique()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['rec.autos', 'comp.sys.mac.hardware', 'comp.graphics', 'sci.space',\n",
              "       'talk.politics.guns', 'sci.med', 'comp.sys.ibm.pc.hardware',\n",
              "       'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc',\n",
              "       'misc.forsale', 'alt.atheism', 'sci.electronics', 'comp.windows.x',\n",
              "       'rec.sport.hockey', 'rec.sport.baseball', 'soc.religion.christian',\n",
              "       'talk.politics.mideast', 'talk.politics.misc', 'sci.crypt'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKiwwwxR5yBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb6a34e6-9ee5-43a4-fcd4-b88f4ed10ce3"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj_9Nybt3ITV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df.content.values.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6foK0djMKyOF",
        "colab_type": "text"
      },
      "source": [
        "### Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj6aGWNw4jCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(sent):\n",
        "    # remove emails\n",
        "    sent = re.sub('\\S*@\\S*\\s?', '', sent)\n",
        "    # remove newline chars\n",
        "    sent = re.sub('\\s+', ' ', sent)\n",
        "    # remove single quotes\n",
        "    sent = re.sub(\"\\'\", \"\", sent)\n",
        "    # converts to lower case tokens and removes tokens that are\n",
        "    # too small & too long. Also remove accent characters & punct\n",
        "    tokens = simple_preprocess(str(sent), deacc=True)\n",
        "    # remove stopwords\n",
        "    stop_free = [i for i in tokens if i not in stop_words]\n",
        "    # lemmatization\n",
        "    lemmatized = [lemma.lemmatize(word) for word in stop_free]\n",
        "    # stemming\n",
        "    normalized = [stemmer.stem(word) for word in lemmatized]\n",
        "    return normalized"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ed0UZV5kmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0ebd745-7ff5-4ec7-c459-db546983b1f1"
      },
      "source": [
        "cleaned_data = [preprocess(doc) for doc in data]\n",
        "cleaned_data[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where',\n",
              " 'thing',\n",
              " 'car',\n",
              " 'nntp',\n",
              " 'post',\n",
              " 'host',\n",
              " 'rac',\n",
              " 'wam',\n",
              " 'umd',\n",
              " 'organ',\n",
              " 'univers',\n",
              " 'maryland',\n",
              " 'colleg',\n",
              " 'park',\n",
              " 'line',\n",
              " 'wonder',\n",
              " 'anyon',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'car',\n",
              " 'saw',\n",
              " 'day',\n",
              " 'door',\n",
              " 'sport',\n",
              " 'car',\n",
              " 'look',\n",
              " 'late',\n",
              " 'earli',\n",
              " 'call',\n",
              " 'bricklin',\n",
              " 'door',\n",
              " 'realli',\n",
              " 'small',\n",
              " 'addit',\n",
              " 'front',\n",
              " 'bumper',\n",
              " 'separ',\n",
              " 'rest',\n",
              " 'bodi',\n",
              " 'know',\n",
              " 'anyon',\n",
              " 'tellm',\n",
              " 'model',\n",
              " 'name',\n",
              " 'engin',\n",
              " 'spec',\n",
              " 'year',\n",
              " 'product',\n",
              " 'car',\n",
              " 'made',\n",
              " 'histori',\n",
              " 'whatev',\n",
              " 'info',\n",
              " 'funki',\n",
              " 'look',\n",
              " 'car',\n",
              " 'pleas',\n",
              " 'mail',\n",
              " 'thank',\n",
              " 'il',\n",
              " 'brought',\n",
              " 'neighborhood',\n",
              " 'lerxst']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3V2cDRVKu6t",
        "colab_type": "text"
      },
      "source": [
        "### Corpus creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji4Bbe9q9_0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dictionary\n",
        "id2word = corpora.Dictionary(cleaned_data)\n",
        "\n",
        "# bag of words\n",
        "corpus = [id2word.doc2bow(text) for text in cleaned_data]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP9yM4-Y-gpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "00645ae1-069d-4358-c538-67b8d894470f"
      },
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('addit', 1),\n",
              "  ('anyon', 2),\n",
              "  ('bodi', 1),\n",
              "  ('bricklin', 1),\n",
              "  ('brought', 1),\n",
              "  ('bumper', 1),\n",
              "  ('call', 1),\n",
              "  ('car', 5),\n",
              "  ('colleg', 1),\n",
              "  ('could', 1),\n",
              "  ('day', 1),\n",
              "  ('door', 2),\n",
              "  ('earli', 1),\n",
              "  ('engin', 1),\n",
              "  ('enlighten', 1),\n",
              "  ('front', 1),\n",
              "  ('funki', 1),\n",
              "  ('histori', 1),\n",
              "  ('host', 1),\n",
              "  ('il', 1),\n",
              "  ('info', 1),\n",
              "  ('know', 1),\n",
              "  ('late', 1),\n",
              "  ('lerxst', 1),\n",
              "  ('line', 1),\n",
              "  ('look', 2),\n",
              "  ('made', 1),\n",
              "  ('mail', 1),\n",
              "  ('maryland', 1),\n",
              "  ('model', 1),\n",
              "  ('name', 1),\n",
              "  ('neighborhood', 1),\n",
              "  ('nntp', 1),\n",
              "  ('organ', 1),\n",
              "  ('park', 1),\n",
              "  ('pleas', 1),\n",
              "  ('post', 1),\n",
              "  ('product', 1),\n",
              "  ('rac', 1),\n",
              "  ('realli', 1),\n",
              "  ('rest', 1),\n",
              "  ('saw', 1),\n",
              "  ('separ', 1),\n",
              "  ('small', 1),\n",
              "  ('spec', 1),\n",
              "  ('sport', 1),\n",
              "  ('tellm', 1),\n",
              "  ('thank', 1),\n",
              "  ('thing', 1),\n",
              "  ('umd', 1),\n",
              "  ('univers', 1),\n",
              "  ('wam', 1),\n",
              "  ('whatev', 1),\n",
              "  ('where', 1),\n",
              "  ('wonder', 1),\n",
              "  ('year', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3euF5umKrHN",
        "colab_type": "text"
      },
      "source": [
        "### LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5s-5Ymb-jhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=20,\n",
        "    random_state=42,\n",
        "    passes=30\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q715JgAD-2h-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a417ac60-3d71-4c7e-f6c7-bd4ff6b2dde0"
      },
      "source": [
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.007*\"new\" + 0.007*\"state\" + 0.006*\"inform\" + 0.006*\"nation\" + '\n",
            "  '0.005*\"govern\" + 0.005*\"research\" + 0.005*\"public\" + 0.004*\"report\" + '\n",
            "  '0.004*\"author\" + 0.004*\"american\"'),\n",
            " (1,\n",
            "  '0.011*\"would\" + 0.009*\"gun\" + 0.008*\"think\" + 0.008*\"dont\" + 0.008*\"go\" + '\n",
            "  '0.007*\"get\" + 0.007*\"like\" + 0.007*\"peopl\" + 0.007*\"one\" + 0.006*\"make\"'),\n",
            " (2,\n",
            "  '0.716*\"ax\" + 0.052*\"max\" + 0.007*\"pl\" + 0.006*\"di\" + 0.005*\"wm\" + '\n",
            "  '0.005*\"ei\" + 0.005*\"tm\" + 0.004*\"bhj\" + 0.004*\"giz\" + 0.003*\"ql\"'),\n",
            " (3,\n",
            "  '0.015*\"write\" + 0.014*\"articl\" + 0.013*\"right\" + 0.011*\"organ\" + '\n",
            "  '0.010*\"govern\" + 0.009*\"law\" + 0.009*\"peopl\" + 0.009*\"line\" + 0.007*\"state\" '\n",
            "  '+ 0.006*\"post\"'),\n",
            " (4,\n",
            "  '0.035*\"line\" + 0.033*\"organ\" + 0.031*\"post\" + 0.024*\"host\" + 0.023*\"nntp\" + '\n",
            "  '0.021*\"univers\" + 0.011*\"articl\" + 0.011*\"distribut\" + 0.010*\"thank\" + '\n",
            "  '0.009*\"repli\"'),\n",
            " (5,\n",
            "  '0.036*\"file\" + 0.019*\"window\" + 0.010*\"line\" + 0.010*\"program\" + '\n",
            "  '0.009*\"imag\" + 0.009*\"organ\" + 0.008*\"gif\" + 0.008*\"font\" + 0.007*\"driver\" '\n",
            "  '+ 0.007*\"printer\"'),\n",
            " (6,\n",
            "  '0.013*\"window\" + 0.009*\"program\" + 0.008*\"use\" + 0.006*\"avail\" + '\n",
            "  '0.006*\"server\" + 0.006*\"system\" + 0.006*\"softwar\" + 0.006*\"imag\" + '\n",
            "  '0.006*\"version\" + 0.006*\"run\"'),\n",
            " (7,\n",
            "  '0.020*\"god\" + 0.017*\"christian\" + 0.011*\"one\" + 0.010*\"exist\" + 0.010*\"say\" '\n",
            "  '+ 0.009*\"believ\" + 0.008*\"bibl\" + 0.007*\"think\" + 0.007*\"question\" + '\n",
            "  '0.007*\"mean\"'),\n",
            " (8,\n",
            "  '0.012*\"one\" + 0.012*\"peopl\" + 0.010*\"would\" + 0.010*\"know\" + 0.010*\"dont\" + '\n",
            "  '0.009*\"write\" + 0.009*\"go\" + 0.009*\"say\" + 0.009*\"like\" + 0.007*\"think\"'),\n",
            " (9,\n",
            "  '0.011*\"car\" + 0.009*\"space\" + 0.006*\"one\" + 0.006*\"articl\" + 0.005*\"write\" '\n",
            "  '+ 0.005*\"like\" + 0.005*\"would\" + 0.005*\"bike\" + 0.005*\"organ\" + '\n",
            "  '0.005*\"line\"'),\n",
            " (10,\n",
            "  '0.012*\"game\" + 0.011*\"year\" + 0.009*\"run\" + 0.008*\"line\" + 0.008*\"organ\" + '\n",
            "  '0.008*\"team\" + 0.007*\"articl\" + 0.007*\"last\" + 0.006*\"write\" + '\n",
            "  '0.006*\"basebal\"'),\n",
            " (11,\n",
            "  '0.029*\"god\" + 0.029*\"jesu\" + 0.019*\"church\" + 0.015*\"christ\" + 0.013*\"sin\" '\n",
            "  '+ 0.009*\"lord\" + 0.009*\"paul\" + 0.008*\"cathol\" + 0.007*\"father\" + '\n",
            "  '0.007*\"holi\"'),\n",
            " (12,\n",
            "  '0.013*\"entri\" + 0.012*\"wire\" + 0.008*\"line\" + 0.007*\"output\" + 0.007*\"file\" '\n",
            "  '+ 0.007*\"ground\" + 0.006*\"need\" + 0.006*\"use\" + 0.006*\"circuit\" + '\n",
            "  '0.006*\"one\"'),\n",
            " (13,\n",
            "  '0.014*\"armenian\" + 0.012*\"israel\" + 0.010*\"isra\" + 0.009*\"jew\" + '\n",
            "  '0.008*\"turkish\" + 0.007*\"peopl\" + 0.007*\"war\" + 0.007*\"arab\" + 0.006*\"kill\" '\n",
            "  '+ 0.005*\"greek\"'),\n",
            " (14,\n",
            "  '0.014*\"ripem\" + 0.010*\"organ\" + 0.008*\"kent\" + 0.007*\"line\" + '\n",
            "  '0.007*\"articl\" + 0.006*\"john\" + 0.005*\"order\" + 0.004*\"candida\" + '\n",
            "  '0.004*\"write\" + 0.004*\"yeast\"'),\n",
            " (15,\n",
            "  '0.018*\"drive\" + 0.013*\"card\" + 0.009*\"system\" + 0.009*\"scsi\" + 0.008*\"disk\" '\n",
            "  '+ 0.008*\"problem\" + 0.008*\"mb\" + 0.008*\"mac\" + 0.007*\"work\" + '\n",
            "  '0.007*\"control\"'),\n",
            " (16,\n",
            "  '0.010*\"organ\" + 0.010*\"line\" + 0.009*\"articl\" + 0.008*\"write\" + '\n",
            "  '0.008*\"scienc\" + 0.007*\"one\" + 0.007*\"post\" + 0.006*\"case\" + 0.006*\"bank\" + '\n",
            "  '0.005*\"point\"'),\n",
            " (17,\n",
            "  '0.022*\"game\" + 0.021*\"team\" + 0.016*\"play\" + 0.014*\"hockey\" + '\n",
            "  '0.012*\"player\" + 0.009*\"nhl\" + 0.008*\"win\" + 0.007*\"pt\" + 0.007*\"season\" + '\n",
            "  '0.007*\"la\"'),\n",
            " (18,\n",
            "  '0.032*\"key\" + 0.016*\"encrypt\" + 0.014*\"chip\" + 0.011*\"secur\" + '\n",
            "  '0.011*\"clipper\" + 0.008*\"use\" + 0.007*\"one\" + 0.007*\"system\" + 0.006*\"bit\" '\n",
            "  '+ 0.006*\"would\"'),\n",
            " (19,\n",
            "  '0.014*\"cx\" + 0.010*\"c_\" + 0.007*\"ei\" + 0.007*\"hz\" + 0.006*\"um\" + 0.006*\"ww\" '\n",
            "  '+ 0.006*\"q\" + 0.005*\"uw\" + 0.005*\"mw\" + 0.005*\"el\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iarRjRBCJSpm",
        "colab_type": "text"
      },
      "source": [
        "### Inferring topics from keywords\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1NC-Sj3JuqX",
        "colab_type": "text"
      },
      "source": [
        "Let's see if we can infer what the topic is about from the Keywords for few of them\n",
        "\n",
        "(18,\n",
        "  '0.032*\"key\" + 0.016*\"encrypt\" + 0.014*\"chip\" + 0.011*\"secur\" + '\n",
        "  '0.011*\"clipper\" + 0.008*\"use\" + 0.007*\"one\" + 0.007*\"system\" + 0.006*\"bit\" '\n",
        "  '+ 0.006*\"would\"') - **crypt**\n",
        "\n",
        "- - -\n",
        "(17,\n",
        "  '0.022*\"game\" + 0.021*\"team\" + 0.016*\"play\" + 0.014*\"hockey\" + '\n",
        "  '0.012*\"player\" + 0.009*\"nhl\" + 0.008*\"win\" + 0.007*\"pt\" + 0.007*\"season\" + '\n",
        "  '0.007*\"la\"') - **hockey**\n",
        "- - -\n",
        "(10,\n",
        "  '0.012*\"game\" + 0.011*\"year\" + 0.009*\"run\" + 0.008*\"line\" + 0.008*\"organ\" + '\n",
        "  '0.008*\"team\" + 0.007*\"articl\" + 0.007*\"last\" + 0.006*\"write\" + '\n",
        "  '0.006*\"basebal\"') - **baseball**\n",
        "\n",
        "\n"
      ]
    }
  ]
}