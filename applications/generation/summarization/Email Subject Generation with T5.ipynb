{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email Subject Generation with T5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYKqMJZO5v5rPPBZ13oajK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fgeneration/applications/generation/summarization/Email%20Subject%20Generation%20with%20T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHZPqxGNOol",
        "colab_type": "text"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW0umIQQNQvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a93d1b5a-c8fd-43d1-a7f2-2892cf960f11"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 778kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7wVsjzDcs7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "6c4a3a9f-29d3-4251-ffe7-0d6798a735db"
      },
      "source": [
        "!pip install nlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/77/9b8a06120e85d8057339a87dca967c8b73280fe463702cc4966c30f43960/nlp-0.3.0-py3-none-any.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 6.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 6.8MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 235kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 286kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 378kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 419kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 471kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 512kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 522kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 573kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 604kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 614kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 655kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 665kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 706kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 716kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 747kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 757kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 788kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 798kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 808kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 839kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 849kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 880kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 890kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 901kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 931kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 942kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 952kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 972kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 983kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0MB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0MB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.0MB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.0MB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.0MB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Installing collected packages: pyarrow, nlp\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed nlp-0.3.0 pyarrow-0.17.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rApsL4NIkU",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPUWFkDyNKFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "20ca4752-2693-4c3d-cd55-199928506e22"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from nlp import load_dataset\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2AYr_nRNoQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25355510-14a7-4f5e-9657-df93f81bed0d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upwwQOJzWezD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cZCu3U5c4ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1e79216-1e9e-48b7-cd49-0b6811c66cf3"
      },
      "source": [
        "dataset = load_dataset('aeslc')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct1F_YEQm5Ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f60ed08-8927-451f-fd2c-42592d78de58"
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'validation', 'test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMlN9gJOQutu",
        "colab_type": "text"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpaGQDmMQyc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3d38f72d-bbc5-4c84-e6ab-3678c29e4f42"
      },
      "source": [
        "train_df = pd.DataFrame(dataset[\"train\"])\n",
        "valid_df = pd.DataFrame(dataset[\"validation\"])\n",
        "test_df = pd.DataFrame(dataset[\"test\"])\n",
        "train_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>email_body</th>\n",
              "      <th>subject_line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Greg/Phillip,  Attached is the Grande Communic...</td>\n",
              "      <td>Service Agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Phillip &amp; Keith  Attached is the first draw re...</td>\n",
              "      <td>Bishops Corner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Your Internet Banking accounts are now setup a...</td>\n",
              "      <td>Internet Banking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To our IBS Customers that are still hanging in...</td>\n",
              "      <td>Internet Banking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phillip Good Morning!\\nI hope you had a wonder...</td>\n",
              "      <td>SMEs for expert stories</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          email_body             subject_line\n",
              "0  Greg/Phillip,  Attached is the Grande Communic...        Service Agreement\n",
              "1  Phillip & Keith  Attached is the first draw re...           Bishops Corner\n",
              "2  Your Internet Banking accounts are now setup a...         Internet Banking\n",
              "3  To our IBS Customers that are still hanging in...         Internet Banking\n",
              "4  Phillip Good Morning!\\nI hope you had a wonder...  SMEs for expert stories"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMy2EJ-KkpTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2da25e14-205e-4789-bd56-18941458d45e"
      },
      "source": [
        "len(train_df), len(valid_df), len(test_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14436, 1960, 1906)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q-f8R4pkwVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "3a5cafd4-a2a0-486f-ca65-5d1c041eff59"
      },
      "source": [
        "print(f\"Email: {train_df.iloc[0]['email_body']} \\nSubject Line: {train_df.iloc[0]['subject_line']}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Email: Greg/Phillip,  Attached is the Grande Communications Service Agreement.\n",
            "The business points can be found in Exhibit C.  I Can get the Non-Disturbance agreement after it has been executed by you and Grande.\n",
            "I will fill in the Legal description of the property one I have received it.\n",
            "Please execute and send to:  Grande Communications, 401 Carlson Circle, San Marcos Texas, 78666 Attention Hunter Williams.\n",
            "<<Bishopscontract.doc>>\n",
            " \n",
            "Subject Line: Service Agreement\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOnOBy7kmF6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "522ff4d8-7991-42da-f86e-58a062cad518"
      },
      "source": [
        "train_df.email_body = \"summarize: \" + train_df.email_body\n",
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>email_body</th>\n",
              "      <th>subject_line</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>summarize: Greg/Phillip,  Attached is the Gran...</td>\n",
              "      <td>Service Agreement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summarize: Phillip &amp; Keith  Attached is the fi...</td>\n",
              "      <td>Bishops Corner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>summarize: Your Internet Banking accounts are ...</td>\n",
              "      <td>Internet Banking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>summarize: To our IBS Customers that are still...</td>\n",
              "      <td>Internet Banking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>summarize: Phillip Good Morning!\\nI hope you h...</td>\n",
              "      <td>SMEs for expert stories</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          email_body             subject_line\n",
              "0  summarize: Greg/Phillip,  Attached is the Gran...        Service Agreement\n",
              "1  summarize: Phillip & Keith  Attached is the fi...           Bishops Corner\n",
              "2  summarize: Your Internet Banking accounts are ...         Internet Banking\n",
              "3  summarize: To our IBS Customers that are still...         Internet Banking\n",
              "4  summarize: Phillip Good Morning!\\nI hope you h...  SMEs for expert stories"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZsuYex2kSv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using only 50% of the data\n",
        "train_size = 0.5\n",
        "valid_size = 0.5\n",
        "\n",
        "train_df = train_df.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
        "valid_df = valid_df.sample(frac=valid_size, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=valid_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "valid_df.email_body = \"summarize: \" + valid_df.email_body\n",
        "test_df.email_body = \"summarize: \" + test_df.email_body"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Y5ajOokipi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e06056d2-8568-49ce-b687-279d44013797"
      },
      "source": [
        "len(train_df), len(valid_df), len(test_df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7218, 980, 953)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5B2QNEsZm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emails = train_df.email_body.values\n",
        "email_lens = [len(email.split()) for email in emails]\n",
        "\n",
        "subjects = train_df.subject_line.values\n",
        "subject_lens = [len(subject.split()) for subject in subjects]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EavEzN2bsoRX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6c5d2742-628b-404e-f749-de81d9257e36"
      },
      "source": [
        "sns.distplot(email_lens)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6f07a18fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i_ImmPTtDMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "97ef9352-278e-4e59-ce37-3847b82a82ef"
      },
      "source": [
        "sns.distplot(subject_lens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6f078162e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5T_hLv0Rgty",
        "colab_type": "text"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQAEJPMYRiL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = \"t5-base\"\n",
        "MODEL_PATH = \"model.pt\"\n",
        "TOKENIZER = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 200\n",
        "SUMMARY_LEN = 30\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 2\n",
        "LR = 1e-4"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFeOkQq7QqQy",
        "colab_type": "text"
      },
      "source": [
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9OGakDYNyPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmailDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, source_len, summary_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = df\n",
        "        self.source_len = source_len\n",
        "        self.summary_len = summary_len\n",
        "        self.email_body = df.email_body\n",
        "        self.subject_line = df.subject_line\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.subject_line)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        email = str(self.email_body[item])\n",
        "        email = \" \".join(email.split())\n",
        "\n",
        "        subject = str(self.subject_line[item])\n",
        "        subject = \" \".join(subject.split())\n",
        "\n",
        "        source = self.tokenizer.encode_plus(\n",
        "            email,\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        target = self.tokenizer.encode_plus(\n",
        "            subject,\n",
        "            max_length=self.summary_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source[\"input_ids\"].flatten(),\n",
        "            \"source_mask\": source[\"attention_mask\"].flatten(),\n",
        "            \"target_ids\": target[\"input_ids\"].flatten(),\n",
        "            \"target_mask\": target[\"attention_mask\"].flatten()\n",
        "        }"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hE_pbITRNGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = EmailDataset(train_df, TOKENIZER, MAX_LEN, SUMMARY_LEN)\n",
        "valid_dataset = EmailDataset(valid_df, TOKENIZER, MAX_LEN, SUMMARY_LEN)\n",
        "test_dataset = EmailDataset(test_df, TOKENIZER, MAX_LEN, SUMMARY_LEN)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rR6qRs7QPR8",
        "colab_type": "text"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOein1F8QTqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "val_data_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDfMiXp1Qj_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06eeea57-4a1e-439d-e999-25cc8b1771d7"
      },
      "source": [
        "# sample check\n",
        "sample = next(iter(train_data_loader))\n",
        "sample['source_ids'].shape, sample['source_mask'].shape, sample['target_ids'].shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 200]), torch.Size([2, 200]), torch.Size([2, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_nbv8k1Q-D9",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOrAYb8pQ-1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0514ba86-90ad-4d5f-86fa-79ae6bc56a07"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY1FVakDXAO4",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahsjZefkRJ-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36dmjzmXKab",
        "colab_type": "text"
      },
      "source": [
        "### Training Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgunb7h6XJqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data_loader, model, tokenizer, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    total_steps = len(data_loader)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ids = batch[\"source_ids\"].to(device)\n",
        "        mask = batch[\"source_mask\"].to(device)\n",
        "\n",
        "        target_ids = batch[\"target_ids\"].to(device)\n",
        "        \n",
        "        y_ids = target_ids[:, :-1].contiguous()\n",
        "        lm_labels = target_ids[:, 1:].clone().detach()\n",
        "        lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            lm_labels=lm_labels\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if idx%100 == 0:\n",
        "            print(f\"Step: {idx}/{total_steps} | Loss: {loss.item()}\")\n",
        "    \n",
        "    return epoch_loss / total_steps"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuwfseE_XPoJ",
        "colab_type": "text"
      },
      "source": [
        "### Validation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLG_2qJAXNZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_loader, model, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_steps = len(data_loader)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(data_loader):\n",
        "            ids = batch[\"source_ids\"].to(device)\n",
        "            mask = batch[\"source_mask\"].to(device)\n",
        "\n",
        "            target_ids = batch[\"target_ids\"].to(device)\n",
        "            \n",
        "            y_ids = target_ids[:, :-1].contiguous()\n",
        "            lm_labels = target_ids[:, 1:].clone().detach()\n",
        "            lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                decoder_input_ids=y_ids,\n",
        "                lm_labels=lm_labels\n",
        "            )\n",
        "\n",
        "            loss = outputs[0]\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if idx%100 == 0:\n",
        "                print(f\"Val Step: {idx}/{total_steps} | Loss: {loss.item()}\")\n",
        "    \n",
        "    return epoch_loss / total_steps"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8otkSXUmXTj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98s2V8WkXe6T",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdwZKD-YttRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52623076-1c51-4c23-a473-ad0e44beb94d"
      },
      "source": [
        "device"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08LzFCXvXeLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6009991a-dc05-4b1b-bd0a-9fddd3f15811"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(train_data_loader, model, TOKENIZER, optimizer, device)\n",
        "    val_loss = evaluate(val_data_loader, model, TOKENIZER, device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_valid_loss:\n",
        "        best_valid_loss = val_loss\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\t Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):5.4f}\")\n",
        "    print(f\"\\t Val Loss: {val_loss:.3f} | Val PPL: {np.exp(val_loss):5.4f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0/3609 | Loss: 9.252239227294922\n",
            "Step: 100/3609 | Loss: 3.096573829650879\n",
            "Step: 200/3609 | Loss: 2.9988081455230713\n",
            "Step: 300/3609 | Loss: 3.623548984527588\n",
            "Step: 400/3609 | Loss: 0.7167368531227112\n",
            "Step: 500/3609 | Loss: 1.7852838039398193\n",
            "Step: 600/3609 | Loss: 5.5049333572387695\n",
            "Step: 700/3609 | Loss: 3.3280117511749268\n",
            "Step: 800/3609 | Loss: 3.317720413208008\n",
            "Step: 900/3609 | Loss: 2.0162787437438965\n",
            "Step: 1000/3609 | Loss: 4.236403465270996\n",
            "Step: 1100/3609 | Loss: 3.0190610885620117\n",
            "Step: 1200/3609 | Loss: 4.0998053550720215\n",
            "Step: 1300/3609 | Loss: 2.6751210689544678\n",
            "Step: 1400/3609 | Loss: 3.631730079650879\n",
            "Step: 1500/3609 | Loss: 4.754125595092773\n",
            "Step: 1600/3609 | Loss: 4.393270015716553\n",
            "Step: 1700/3609 | Loss: 2.711280345916748\n",
            "Step: 1800/3609 | Loss: 6.806175231933594\n",
            "Step: 1900/3609 | Loss: 1.9674499034881592\n",
            "Step: 2000/3609 | Loss: 4.142590045928955\n",
            "Step: 2100/3609 | Loss: 4.876557350158691\n",
            "Step: 2200/3609 | Loss: 0.8874102830886841\n",
            "Step: 2300/3609 | Loss: 6.20208215713501\n",
            "Step: 2400/3609 | Loss: 1.575003743171692\n",
            "Step: 2500/3609 | Loss: 6.54710054397583\n",
            "Step: 2600/3609 | Loss: 3.2135446071624756\n",
            "Step: 2700/3609 | Loss: 5.439290523529053\n",
            "Step: 2800/3609 | Loss: 3.563314437866211\n",
            "Step: 2900/3609 | Loss: 1.105194330215454\n",
            "Step: 3000/3609 | Loss: 4.550641059875488\n",
            "Step: 3100/3609 | Loss: 2.747737169265747\n",
            "Step: 3200/3609 | Loss: 4.621835708618164\n",
            "Step: 3300/3609 | Loss: 4.37043571472168\n",
            "Step: 3400/3609 | Loss: 4.477697372436523\n",
            "Step: 3500/3609 | Loss: 3.0624191761016846\n",
            "Step: 3600/3609 | Loss: 3.568286418914795\n",
            "Val Step: 0/490 | Loss: 2.568168878555298\n",
            "Val Step: 100/490 | Loss: 4.012210369110107\n",
            "Val Step: 200/490 | Loss: 1.2476615905761719\n",
            "Val Step: 300/490 | Loss: 4.734692096710205\n",
            "Val Step: 400/490 | Loss: 3.126664876937866\n",
            "Epoch: 01 | Epoch Time: 25m 20s\n",
            "\t Train Loss: 3.504 | Train PPL: 33.2587\n",
            "\t Val Loss: 2.746 | Val PPL: 15.5810\n",
            "Step: 0/3609 | Loss: 2.4789187908172607\n",
            "Step: 100/3609 | Loss: 4.299111366271973\n",
            "Step: 200/3609 | Loss: 1.082140326499939\n",
            "Step: 300/3609 | Loss: 2.619184732437134\n",
            "Step: 400/3609 | Loss: 3.4625768661499023\n",
            "Step: 500/3609 | Loss: 5.449767112731934\n",
            "Step: 600/3609 | Loss: 3.134702205657959\n",
            "Step: 700/3609 | Loss: 3.6931283473968506\n",
            "Step: 800/3609 | Loss: 1.1749241352081299\n",
            "Step: 900/3609 | Loss: 3.1866908073425293\n",
            "Step: 1000/3609 | Loss: 2.3054957389831543\n",
            "Step: 1100/3609 | Loss: 0.4434370696544647\n",
            "Step: 1200/3609 | Loss: 0.21530033648014069\n",
            "Step: 1300/3609 | Loss: 5.751262187957764\n",
            "Step: 1400/3609 | Loss: 0.9524257779121399\n",
            "Step: 1500/3609 | Loss: 0.22084683179855347\n",
            "Step: 1600/3609 | Loss: 2.70463228225708\n",
            "Step: 1700/3609 | Loss: 2.6140828132629395\n",
            "Step: 1800/3609 | Loss: 3.64851450920105\n",
            "Step: 1900/3609 | Loss: 1.4952248334884644\n",
            "Step: 2000/3609 | Loss: 5.561173915863037\n",
            "Step: 2100/3609 | Loss: 2.0675694942474365\n",
            "Step: 2200/3609 | Loss: 3.353046178817749\n",
            "Step: 2300/3609 | Loss: 0.3480207622051239\n",
            "Step: 2400/3609 | Loss: 5.953916549682617\n",
            "Step: 2500/3609 | Loss: 2.8817615509033203\n",
            "Step: 2600/3609 | Loss: 3.339625120162964\n",
            "Step: 2700/3609 | Loss: 3.6546857357025146\n",
            "Step: 2800/3609 | Loss: 3.9166982173919678\n",
            "Step: 2900/3609 | Loss: 3.023189067840576\n",
            "Step: 3000/3609 | Loss: 3.3040378093719482\n",
            "Step: 3100/3609 | Loss: 0.9463804364204407\n",
            "Step: 3200/3609 | Loss: 4.626529693603516\n",
            "Step: 3300/3609 | Loss: 0.9425652027130127\n",
            "Step: 3400/3609 | Loss: 0.009977355599403381\n",
            "Step: 3500/3609 | Loss: 1.763372540473938\n",
            "Step: 3600/3609 | Loss: 4.571835517883301\n",
            "Val Step: 0/490 | Loss: 2.405425786972046\n",
            "Val Step: 100/490 | Loss: 4.287723541259766\n",
            "Val Step: 200/490 | Loss: 0.7909502387046814\n",
            "Val Step: 300/490 | Loss: 4.532944202423096\n",
            "Val Step: 400/490 | Loss: 3.0937130451202393\n",
            "Epoch: 02 | Epoch Time: 25m 21s\n",
            "\t Train Loss: 2.578 | Train PPL: 13.1754\n",
            "\t Val Loss: 2.643 | Val PPL: 14.0608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys1kbG4mfGnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef637144-4dc3-4d34-def8-e1b8fd0919d5"
      },
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ijp5Cmm6a-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "48850710-1ca9-49d5-827a-ab775975b2c6"
      },
      "source": [
        "test_loss = evaluate(test_data_loader, model, TOKENIZER, device)\n",
        "print(f\"\\t Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):5.4f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val Step: 0/477 | Loss: 1.4821603298187256\n",
            "Val Step: 100/477 | Loss: 3.4901652336120605\n",
            "Val Step: 200/477 | Loss: 1.9917783737182617\n",
            "Val Step: 300/477 | Loss: 2.3270175457000732\n",
            "Val Step: 400/477 | Loss: 1.977234959602356\n",
            "\t Test Loss: 2.790 | Test PPL: 16.2773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajw3V84qhtI3",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1fZ__wrhuuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(model, email, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    email = \"summarize: \" + email\n",
        "\n",
        "    source = tokenizer.encode_plus(\n",
        "        email,\n",
        "        max_length=MAX_LEN,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        ids = source[\"input_ids\"].to(device)\n",
        "        mask = source[\"attention_mask\"].to(device)\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            max_length=10,\n",
        "            num_beams=2,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "    return summary[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugW--OxirQyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "email = \"\"\"\n",
        "Celeste,  I was under the impression I sent you a message regarding Brad Romine but I cannot find a copy in my msg folder.\n",
        "There might have been a glitch in the  CC-mail and the message did not go through and wasn't  stored.\n",
        "I wanted to make the following points:  1.\n",
        "Brad will not show up on March 15.\n",
        "He is still working on his dot-com  business and wants to pursue this opportunity.\n",
        "2.\n",
        "My recommendation is that we should draw a line in the sand.\n",
        "Either Brad  or a stipend refund check should show up on March 15.\n",
        "3.\n",
        "I told Brad that  a failure to show up on March 15 will not imperil his  future employment opportunities with Enron.\n",
        "We just need clarity and ability to plan our human resource  needs.\n",
        "If he decides to re-apply at some point in the future, we shall not hold his  decision  to pursue him  entrepreneurial plans against him.\n",
        "Please, let me  know what you think.\n",
        "Vince\n",
        "\"\"\""
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pm4A3LgsDQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0963eb9-787d-437d-cbf9-58c75a75899b"
      },
      "source": [
        "email = \" \".join(email.split()).strip()\n",
        "subject = inference(model, email, TOKENIZER, device)\n",
        "print(subject)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Brad Romine - March 15th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qo2xxHjCUd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "356953b8-e7e8-44ae-9088-bd466415ffd1"
      },
      "source": [
        "dataset['validation'][800]['email_body']"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Celeste,  I was under the impression I sent you a message regarding Brad Romine but I cannot find a copy in my msg folder.\\nThere might have been a glitch in the  CC-mail and the message did not go through and wasn't  stored.\\nI wanted to make the following points:  1.\\nBrad will not show up on March 15.\\nHe is still working on his dot-com  business and wants to pursue this opportunity.\\n2.\\nMy recommendation is that we should draw a line in the sand.\\nEither Brad  or a stipend refund check should show up on March 15.\\n3.\\nI told Brad that  a failure to show up on March 15 will not imperil his  future employment opportunities with Enron.\\nWe just need clarity and ability to plan our human resource  needs.\\nIf he decides to re-apply at some point in the future, we shall not hold his  decision  to pursue him  entrepreneurial plans against him.\\nPlease, let me  know what you think.\\nVince\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPopYCn64o0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fa01756-b9f3-412b-9e1b-019bd7a8762a"
      },
      "source": [
        "dataset['validation'][800]['subject_line']"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Brad Romine\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGqVWesy7egY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}